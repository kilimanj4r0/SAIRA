{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a solution using Retrieval Augmented Generation with LLM based on LlamaIndex framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Flatten all text files into raw folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def copy_text_files(source_dir, destination_dir):\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.makedirs(destination_dir)\n",
    "\n",
    "    # Traverse the source directory\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            # Check if the file has a .txt extension (you can modify this condition)\n",
    "            if file.endswith(\".txt\") or file.endswith(\".md\"):\n",
    "                source_path = os.path.join(root, file)\n",
    "                destination_path = os.path.join(destination_dir, 'raw_' + root.split('/')[-1] + '_' + file)\n",
    "\n",
    "                # Copy the file to the destination directory\n",
    "                shutil.copy2(source_path, destination_path)\n",
    "                # print(f\"Copied: {file}\")\n",
    "\n",
    "# Replace 'source_directory' and 'destination_directory' with your actual paths\n",
    "copy_text_files('../data/documents/', '../data/raw')\n",
    "copy_text_files('../data/documents/edu-wiki', '../data/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn on logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.WARNING)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import os\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores import ChromaVectorStore, SimpleVectorStore\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = \"../data/raw\"\n",
    "documents = SimpleDirectoryReader(docs_dir).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create service context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "\n",
    "# llm_model = \"llama2:13b\"\n",
    "llm_model = \"mistral\"\n",
    "# llm_model = \"orca2:13b\"\n",
    "# llm_model = \"vicuna:13b-16k\"\n",
    "llm = Ollama(model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.file import SimpleFileNodeParser\n",
    "# from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "# node_parser = SimpleFileNodeParser()\n",
    "# node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import PromptHelper\n",
    "\n",
    "\n",
    "# prompt_helper = PromptHelper(\n",
    "    # context_window=4096,\n",
    "    # num_output=256,\n",
    "    # chunk_overlap_ratio=0.1,\n",
    "    # chunk_size_limit=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-27 16:06:59,648] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")  # Best model for retrieval according to MTEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/vladimir-torch/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    # embed_model='local:BAAI/bge-large-en-v1.5',  # Best model for retrieval according to MTEB\n",
    "    # embed_model='local',\n",
    "    embed_model=embed_model,\n",
    "    # node_parser=node_parser,\n",
    "    # prompt_helper=prompt_helper,\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create storage context (for ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chromadb.api.client.Client at 0x7f59742ae860>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persist_dir = \"../storage/chromadb\"\n",
    "persist_dir = os.path.abspath(persist_dir)\n",
    "\n",
    "db = chromadb.PersistentClient(path=persist_dir, settings=chromadb.Settings(allow_reset=True))\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_ef = chromadb.utils.embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=\"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\",\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [],\n",
       " 'embeddings': [],\n",
       " 'metadatas': [],\n",
       " 'documents': [],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_collection = db.get_or_create_collection(\"saira\")\n",
    "chroma_collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stores_text': True,\n",
       " 'is_embedding_query': True,\n",
       " 'flat_metadata': True,\n",
       " 'collection_name': None,\n",
       " 'host': None,\n",
       " 'port': None,\n",
       " 'ssl': False,\n",
       " 'headers': None,\n",
       " 'persist_dir': None,\n",
       " 'collection_kwargs': {},\n",
       " 'class_name': 'ChromaVectorStore'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "vector_store.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "storage_context.vector_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7f5ad85d6a40>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(persist_dir + '/index_store.json'):  # Load\n",
    "    print('Loading index')\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        service_context=service_context,\n",
    "    )\n",
    "else:  # Create\n",
    "    print('Creating index')\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create storage context (SimpleVectorStore) and (or load) index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading storage context\n",
      "Loading index\n"
     ]
    }
   ],
   "source": [
    "persist_dir = \"../storage/simple/\"\n",
    "persist_dir = str(Path(persist_dir).resolve())\n",
    "\n",
    "if os.path.exists(persist_dir + '/index_store.json'):  # Load\n",
    "    old_name = '/default__vector_store.json'\n",
    "    new_name = '/vector_store.json'\n",
    "    if os.path.exists(persist_dir + old_name):\n",
    "        os.rename(persist_dir + old_name, persist_dir + new_name)\n",
    "        print(f\"File renamed from '{old_name}' to '{new_name}'.\")\n",
    "    print('Loading storage context')\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=SimpleVectorStore.from_persist_dir(persist_dir=persist_dir),\n",
    "        persist_dir=persist_dir,\n",
    "    )\n",
    "    print('Loading index')\n",
    "    index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "else:  # Create\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=SimpleVectorStore(),\n",
    "    )\n",
    "    print('Creating index')\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    print('Persisting index')\n",
    "    storage_context.persist(persist_dir=persist_dir)\n",
    "    index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create storage context (FAISS) and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File renamed from '/default__vector_store.json' to '/vector_store.json'.\n",
      "Loading storage context\n",
      "Loading index\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "\n",
    "d = 1024  # BAAI/bge-large-en-v1.5 embedding size\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "persist_dir = \"../storage/faiss/\"\n",
    "persist_dir = str(Path(persist_dir).resolve())\n",
    "\n",
    "if os.path.exists(persist_dir + '/index_store.json'):  # Load\n",
    "    old_name = '/default__vector_store.json'\n",
    "    new_name = '/vector_store.json'\n",
    "    if os.path.exists(persist_dir + old_name):\n",
    "        os.rename(persist_dir + old_name, persist_dir + new_name)\n",
    "        print(f\"File renamed from '{old_name}' to '{new_name}'.\")\n",
    "    print('Loading storage context')\n",
    "    vector_store = FaissVectorStore.from_persist_dir(persist_dir)\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=vector_store, persist_dir=persist_dir\n",
    "    )\n",
    "    print('Loading index')\n",
    "    index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "else:  # Create\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    print('Creating index')\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        service_context=service_context,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    print('Persisting index')\n",
    "    index.storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create query engine (retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_default = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_sum = index.as_query_engine(similarity_top_k=10, response_mode=\"tree_summarize\", streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import KeywordNodePostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "node_postprocessors = [\n",
    "    KeywordNodePostprocessor(\n",
    "        required_keywords=[\"Combinator\"], exclude_keywords=[\n",
    "            \"Based on the information provided\", \n",
    "            \"Without any prior knowledge\",\n",
    "            \"Based on the information provided from the multiple sources\",\n",
    "            \"\"\n",
    "            ]\n",
    "    )\n",
    "]\n",
    "\n",
    "query_engine_post = RetrieverQueryEngine.from_args(\n",
    "    retriever, node_postprocessors=node_postprocessors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a student club, you can follow these steps:\n",
      "\n",
      "1. Visit the campuslife.innopolis.ru/clubs page.\n",
      "2. Click on the \"Create my Club\" button.\n",
      "3. Fill out the necessary information about your club, such as its name, purpose, and activities.\n",
      "4. Provide any additional details or requirements for membership.\n",
      "5. Submit the form to create your student club.\n",
      "\n",
      "For questions or further assistance, you can contact Evgeniia Dyuzhakova at @janedyuzha_dyuzha.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine_default.query(\"How to create a student club?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innopoints are a motivation and reward system for students who actively participate in extracurricular activities at IU. They can be earned through volunteering and managing student clubs, among other activities. Innopoints can be exchanged for various rewards such as branded merchandise, monthly accommodation, and meals. The InnoStore is a platform where students can redeem their innopoints for these rewards.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine_default.query(\"What are Innopoints?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context information does not mention anything about a dean or their contact information.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine_default.query(\"Who is dean and how to contact their?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It appears that \"innopoints\" may refer to points assigned to students as part of an educational system or program at Innopolis University. However, without more context, it is difficult to provide a specific definition or explanation for what these points represent or how they are awarded. If you could provide more information about the system in question, I would be happy to try and help answer your query.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine_sum.query(\"What are innopoints?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a student club at Innopolis University, you can follow these steps:\n",
      "1. Go to the campuslife.innopolis.ru/clubs website.\n",
      "2. Click on the \"Create my Club\" button.\n",
      "3. Fill out the necessary information for your club, such as its name, description, and purpose.\n",
      "4. Submit your application and wait for approval from the university administration.\n",
      "5. Once approved, you can start organizing activities and events for your club.\n",
      "6. To promote your club, you can use various resources provided by the university, such as audience, basic inventory/equipment, and promotion support.\n",
      "7. You can also participate in Club League, where students earn points for various criteria. The top 20 clubs at the end of each semester receive a budget that they can spend on relevant purchases or services, and the leaders of these clubs can claim points for a Higher Scholarship every semester and innopoints in the end of academic year.\n",
      "8. Joining a student club will give you the opportunity to learn more about yourself, develop soft skills, get networking opportunities, gain practical experience, engage with diverse groups of people, take a break from your studies, expand your CV, and have fun.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine_sum.query(\"How to create a student club?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def generate_answer(question, qe):\n",
    "    response = qe.query(question)\n",
    "    return response.response\n",
    "\n",
    "\n",
    "def add_generated_field(json_data, qe):\n",
    "    for item in json_data:\n",
    "        question = item['question']\n",
    "        generated_answer = generate_answer(question, qe)\n",
    "        item['generated_answer'] = generated_answer\n",
    "    return json_data\n",
    "\n",
    "def add_generated_field_to_directory(input_directory, output_directory, qe):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Iterate through each JSON file in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            input_filepath = os.path.join(input_directory, filename)\n",
    "            output_filepath = os.path.join(output_directory, filename)\n",
    "            if os.path.isfile(output_filepath):\n",
    "                continue\n",
    "\n",
    "            # Load JSON data from the input file\n",
    "            with open(input_filepath, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "\n",
    "            # Add the 'generated' field using the language model\n",
    "            json_data_with_generated = add_generated_field(json_data, qe)\n",
    "\n",
    "            # Save the updated JSON data to the output file\n",
    "            with open(output_filepath, 'w') as output_file:\n",
    "                json.dump(json_data_with_generated, output_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '../data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/faiss-mistral-7b-emb-large-default'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/faiss-mistral-7b-emb-large-summarize'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/simple-vicuna-13b-16k-emb-large-default'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/simple-vicuna-13b-16k-emb-large-summarize'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/simple-llama2-13b-emb-large-summarize'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/simple-llama2-13b-emb-large-default'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/mistral-instruct-7b-emb-large-summarize'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/mistral-instruct-7b-emb-large-default'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/simple-orca2-13b-emb-large-summarize'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '../data/results/simple-orca2-13b-emb-large-default'\n",
    "add_generated_field_to_directory(input_directory, output_directory, query_engine_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vladimir-torch",
   "language": "python",
   "name": "vladimir-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
